{
  "best_global_step": null,
  "best_metric": null,
  "best_model_checkpoint": null,
  "epoch": 0.9999027584195002,
  "eval_steps": 500,
  "global_step": 1928,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.005186217626657158,
      "grad_norm": 4.9525861740112305,
      "learning_rate": 0.0001991701244813278,
      "loss": 4.8069,
      "step": 10
    },
    {
      "epoch": 0.010372435253314317,
      "grad_norm": 5.0489115715026855,
      "learning_rate": 0.00019813278008298758,
      "loss": 3.9766,
      "step": 20
    },
    {
      "epoch": 0.015558652879971475,
      "grad_norm": 4.347157001495361,
      "learning_rate": 0.00019709543568464732,
      "loss": 3.7553,
      "step": 30
    },
    {
      "epoch": 0.020744870506628633,
      "grad_norm": 4.700374603271484,
      "learning_rate": 0.00019605809128630706,
      "loss": 3.7805,
      "step": 40
    },
    {
      "epoch": 0.025931088133285794,
      "grad_norm": 3.586142063140869,
      "learning_rate": 0.0001950207468879668,
      "loss": 3.7628,
      "step": 50
    },
    {
      "epoch": 0.03111730575994295,
      "grad_norm": 4.109106063842773,
      "learning_rate": 0.00019398340248962656,
      "loss": 3.7947,
      "step": 60
    },
    {
      "epoch": 0.03630352338660011,
      "grad_norm": 3.6892330646514893,
      "learning_rate": 0.00019294605809128633,
      "loss": 3.7597,
      "step": 70
    },
    {
      "epoch": 0.04148974101325727,
      "grad_norm": 3.924168348312378,
      "learning_rate": 0.00019190871369294607,
      "loss": 3.7128,
      "step": 80
    },
    {
      "epoch": 0.04667595863991443,
      "grad_norm": 3.8768794536590576,
      "learning_rate": 0.00019087136929460584,
      "loss": 3.7677,
      "step": 90
    },
    {
      "epoch": 0.05186217626657159,
      "grad_norm": 4.245476722717285,
      "learning_rate": 0.00018983402489626555,
      "loss": 3.6591,
      "step": 100
    },
    {
      "epoch": 0.057048393893228744,
      "grad_norm": 4.156820297241211,
      "learning_rate": 0.00018879668049792532,
      "loss": 3.7008,
      "step": 110
    },
    {
      "epoch": 0.0622346115198859,
      "grad_norm": 3.551711320877075,
      "learning_rate": 0.00018775933609958508,
      "loss": 3.6433,
      "step": 120
    },
    {
      "epoch": 0.06742082914654306,
      "grad_norm": 4.592119216918945,
      "learning_rate": 0.00018672199170124482,
      "loss": 3.6549,
      "step": 130
    },
    {
      "epoch": 0.07260704677320022,
      "grad_norm": 4.189964294433594,
      "learning_rate": 0.0001856846473029046,
      "loss": 3.6446,
      "step": 140
    },
    {
      "epoch": 0.07779326439985738,
      "grad_norm": 5.257565021514893,
      "learning_rate": 0.00018464730290456433,
      "loss": 3.6357,
      "step": 150
    },
    {
      "epoch": 0.08297948202651453,
      "grad_norm": 4.643281936645508,
      "learning_rate": 0.00018360995850622407,
      "loss": 3.5173,
      "step": 160
    },
    {
      "epoch": 0.0881656996531717,
      "grad_norm": 5.500041961669922,
      "learning_rate": 0.00018257261410788383,
      "loss": 3.7011,
      "step": 170
    },
    {
      "epoch": 0.09335191727982886,
      "grad_norm": 5.672019958496094,
      "learning_rate": 0.00018153526970954357,
      "loss": 3.6049,
      "step": 180
    },
    {
      "epoch": 0.09853813490648601,
      "grad_norm": 4.278233528137207,
      "learning_rate": 0.00018049792531120334,
      "loss": 3.5841,
      "step": 190
    },
    {
      "epoch": 0.10372435253314317,
      "grad_norm": 4.127755641937256,
      "learning_rate": 0.00017946058091286308,
      "loss": 3.5385,
      "step": 200
    },
    {
      "epoch": 0.10891057015980032,
      "grad_norm": 5.372951984405518,
      "learning_rate": 0.00017842323651452285,
      "loss": 3.5637,
      "step": 210
    },
    {
      "epoch": 0.11409678778645749,
      "grad_norm": 5.241393566131592,
      "learning_rate": 0.00017738589211618256,
      "loss": 3.5721,
      "step": 220
    },
    {
      "epoch": 0.11928300541311465,
      "grad_norm": 4.975128650665283,
      "learning_rate": 0.00017634854771784233,
      "loss": 3.4815,
      "step": 230
    },
    {
      "epoch": 0.1244692230397718,
      "grad_norm": 4.1656365394592285,
      "learning_rate": 0.0001753112033195021,
      "loss": 3.5489,
      "step": 240
    },
    {
      "epoch": 0.12965544066642898,
      "grad_norm": 4.210082530975342,
      "learning_rate": 0.00017427385892116183,
      "loss": 3.4999,
      "step": 250
    },
    {
      "epoch": 0.13484165829308611,
      "grad_norm": 4.415011405944824,
      "learning_rate": 0.0001732365145228216,
      "loss": 3.507,
      "step": 260
    },
    {
      "epoch": 0.14002787591974328,
      "grad_norm": 3.4295008182525635,
      "learning_rate": 0.00017219917012448134,
      "loss": 3.5804,
      "step": 270
    },
    {
      "epoch": 0.14521409354640044,
      "grad_norm": 4.4095845222473145,
      "learning_rate": 0.00017116182572614108,
      "loss": 3.5354,
      "step": 280
    },
    {
      "epoch": 0.1504003111730576,
      "grad_norm": 4.575802803039551,
      "learning_rate": 0.00017012448132780084,
      "loss": 3.5487,
      "step": 290
    },
    {
      "epoch": 0.15558652879971477,
      "grad_norm": 4.1471638679504395,
      "learning_rate": 0.00016908713692946058,
      "loss": 3.5378,
      "step": 300
    },
    {
      "epoch": 0.1607727464263719,
      "grad_norm": 4.14188814163208,
      "learning_rate": 0.00016804979253112035,
      "loss": 3.5558,
      "step": 310
    },
    {
      "epoch": 0.16595896405302907,
      "grad_norm": 22.432659149169922,
      "learning_rate": 0.00016732365145228216,
      "loss": 3.5398,
      "step": 320
    },
    {
      "epoch": 0.17114518167968623,
      "grad_norm": 4.379884719848633,
      "learning_rate": 0.00016639004149377596,
      "loss": 3.5458,
      "step": 330
    },
    {
      "epoch": 0.1763313993063434,
      "grad_norm": 3.8396687507629395,
      "learning_rate": 0.0001653526970954357,
      "loss": 3.4516,
      "step": 340
    },
    {
      "epoch": 0.18151761693300056,
      "grad_norm": 3.9184818267822266,
      "learning_rate": 0.00016431535269709543,
      "loss": 3.4468,
      "step": 350
    },
    {
      "epoch": 0.18670383455965772,
      "grad_norm": 4.294443607330322,
      "learning_rate": 0.0001632780082987552,
      "loss": 3.5378,
      "step": 360
    },
    {
      "epoch": 0.19189005218631486,
      "grad_norm": 4.687819957733154,
      "learning_rate": 0.00016224066390041494,
      "loss": 3.5917,
      "step": 370
    },
    {
      "epoch": 0.19707626981297202,
      "grad_norm": 3.6739423274993896,
      "learning_rate": 0.0001612033195020747,
      "loss": 3.5004,
      "step": 380
    },
    {
      "epoch": 0.20226248743962918,
      "grad_norm": 3.834526777267456,
      "learning_rate": 0.00016016597510373445,
      "loss": 3.4874,
      "step": 390
    },
    {
      "epoch": 0.20744870506628635,
      "grad_norm": 4.226676940917969,
      "learning_rate": 0.0001591286307053942,
      "loss": 3.4572,
      "step": 400
    },
    {
      "epoch": 0.2126349226929435,
      "grad_norm": 4.198732376098633,
      "learning_rate": 0.00015809128630705395,
      "loss": 3.471,
      "step": 410
    },
    {
      "epoch": 0.21782114031960065,
      "grad_norm": 3.949324369430542,
      "learning_rate": 0.0001570539419087137,
      "loss": 3.6471,
      "step": 420
    },
    {
      "epoch": 0.2230073579462578,
      "grad_norm": 3.7007176876068115,
      "learning_rate": 0.00015601659751037346,
      "loss": 3.5118,
      "step": 430
    },
    {
      "epoch": 0.22819357557291498,
      "grad_norm": 5.782463550567627,
      "learning_rate": 0.0001549792531120332,
      "loss": 3.5256,
      "step": 440
    },
    {
      "epoch": 0.23337979319957214,
      "grad_norm": 4.68153715133667,
      "learning_rate": 0.00015394190871369297,
      "loss": 3.4943,
      "step": 450
    },
    {
      "epoch": 0.2385660108262293,
      "grad_norm": 3.8922171592712402,
      "learning_rate": 0.0001529045643153527,
      "loss": 3.5169,
      "step": 460
    },
    {
      "epoch": 0.24375222845288647,
      "grad_norm": 4.459687232971191,
      "learning_rate": 0.00015186721991701244,
      "loss": 3.4978,
      "step": 470
    },
    {
      "epoch": 0.2489384460795436,
      "grad_norm": 3.6913695335388184,
      "learning_rate": 0.0001508298755186722,
      "loss": 3.4162,
      "step": 480
    },
    {
      "epoch": 0.2541246637062008,
      "grad_norm": 4.520052433013916,
      "learning_rate": 0.00014979253112033195,
      "loss": 3.4819,
      "step": 490
    },
    {
      "epoch": 0.25931088133285796,
      "grad_norm": 4.335641860961914,
      "learning_rate": 0.00014875518672199172,
      "loss": 3.4751,
      "step": 500
    },
    {
      "epoch": 0.26449709895951506,
      "grad_norm": 5.539297103881836,
      "learning_rate": 0.00014771784232365146,
      "loss": 3.4279,
      "step": 510
    },
    {
      "epoch": 0.26968331658617223,
      "grad_norm": 3.61137318611145,
      "learning_rate": 0.0001466804979253112,
      "loss": 3.5203,
      "step": 520
    },
    {
      "epoch": 0.2748695342128294,
      "grad_norm": 3.4842288494110107,
      "learning_rate": 0.00014564315352697096,
      "loss": 3.436,
      "step": 530
    },
    {
      "epoch": 0.28005575183948656,
      "grad_norm": 4.078292369842529,
      "learning_rate": 0.0001446058091286307,
      "loss": 3.3895,
      "step": 540
    },
    {
      "epoch": 0.2852419694661437,
      "grad_norm": 4.270907878875732,
      "learning_rate": 0.00014356846473029047,
      "loss": 3.3772,
      "step": 550
    },
    {
      "epoch": 0.2904281870928009,
      "grad_norm": 3.910621404647827,
      "learning_rate": 0.0001425311203319502,
      "loss": 3.4274,
      "step": 560
    },
    {
      "epoch": 0.29561440471945805,
      "grad_norm": 4.108937740325928,
      "learning_rate": 0.00014149377593360998,
      "loss": 3.4798,
      "step": 570
    },
    {
      "epoch": 0.3008006223461152,
      "grad_norm": 4.229154586791992,
      "learning_rate": 0.00014045643153526972,
      "loss": 3.524,
      "step": 580
    },
    {
      "epoch": 0.3059868399727724,
      "grad_norm": 4.237688064575195,
      "learning_rate": 0.00013941908713692945,
      "loss": 3.4015,
      "step": 590
    },
    {
      "epoch": 0.31117305759942954,
      "grad_norm": 4.924187183380127,
      "learning_rate": 0.00013838174273858922,
      "loss": 3.4687,
      "step": 600
    },
    {
      "epoch": 0.3163592752260867,
      "grad_norm": 3.8218493461608887,
      "learning_rate": 0.00013734439834024896,
      "loss": 3.3592,
      "step": 610
    },
    {
      "epoch": 0.3215454928527438,
      "grad_norm": 4.054014682769775,
      "learning_rate": 0.00013630705394190873,
      "loss": 3.4312,
      "step": 620
    },
    {
      "epoch": 0.32673171047940097,
      "grad_norm": 5.64841365814209,
      "learning_rate": 0.00013526970954356847,
      "loss": 3.4016,
      "step": 630
    },
    {
      "epoch": 0.33191792810605814,
      "grad_norm": 4.115253448486328,
      "learning_rate": 0.0001342323651452282,
      "loss": 3.4563,
      "step": 640
    },
    {
      "epoch": 0.3371041457327153,
      "grad_norm": 4.213245391845703,
      "learning_rate": 0.00013319502074688797,
      "loss": 3.4308,
      "step": 650
    },
    {
      "epoch": 0.34229036335937246,
      "grad_norm": 5.072506904602051,
      "learning_rate": 0.0001321576763485477,
      "loss": 3.539,
      "step": 660
    },
    {
      "epoch": 0.3474765809860296,
      "grad_norm": 5.1410603523254395,
      "learning_rate": 0.00013112033195020748,
      "loss": 3.5074,
      "step": 670
    },
    {
      "epoch": 0.3526627986126868,
      "grad_norm": 4.4331231117248535,
      "learning_rate": 0.00013008298755186722,
      "loss": 3.4524,
      "step": 680
    },
    {
      "epoch": 0.35784901623934395,
      "grad_norm": 3.628692626953125,
      "learning_rate": 0.00012904564315352699,
      "loss": 3.6042,
      "step": 690
    },
    {
      "epoch": 0.3630352338660011,
      "grad_norm": 4.364852428436279,
      "learning_rate": 0.00012800829875518673,
      "loss": 3.4805,
      "step": 700
    },
    {
      "epoch": 0.3682214514926583,
      "grad_norm": 4.237495422363281,
      "learning_rate": 0.00012697095435684646,
      "loss": 3.4541,
      "step": 710
    },
    {
      "epoch": 0.37340766911931544,
      "grad_norm": 4.736851215362549,
      "learning_rate": 0.00012593360995850623,
      "loss": 3.3248,
      "step": 720
    },
    {
      "epoch": 0.37859388674597255,
      "grad_norm": 4.041003227233887,
      "learning_rate": 0.00012489626556016597,
      "loss": 3.4618,
      "step": 730
    },
    {
      "epoch": 0.3837801043726297,
      "grad_norm": 4.290571212768555,
      "learning_rate": 0.00012385892116182574,
      "loss": 3.4161,
      "step": 740
    },
    {
      "epoch": 0.3889663219992869,
      "grad_norm": 5.019681453704834,
      "learning_rate": 0.00012282157676348548,
      "loss": 3.3963,
      "step": 750
    },
    {
      "epoch": 0.39415253962594404,
      "grad_norm": 4.3500776290893555,
      "learning_rate": 0.00012178423236514522,
      "loss": 3.4076,
      "step": 760
    },
    {
      "epoch": 0.3993387572526012,
      "grad_norm": 3.781613826751709,
      "learning_rate": 0.00012074688796680498,
      "loss": 3.4669,
      "step": 770
    },
    {
      "epoch": 0.40452497487925837,
      "grad_norm": 4.298722743988037,
      "learning_rate": 0.00011970954356846474,
      "loss": 3.4516,
      "step": 780
    },
    {
      "epoch": 0.40971119250591553,
      "grad_norm": 5.29981803894043,
      "learning_rate": 0.00011867219917012449,
      "loss": 3.4171,
      "step": 790
    },
    {
      "epoch": 0.4148974101325727,
      "grad_norm": 5.309215068817139,
      "learning_rate": 0.00011763485477178424,
      "loss": 3.3872,
      "step": 800
    },
    {
      "epoch": 0.42008362775922986,
      "grad_norm": 4.713390827178955,
      "learning_rate": 0.000116597510373444,
      "loss": 3.46,
      "step": 810
    },
    {
      "epoch": 0.425269845385887,
      "grad_norm": 5.228548049926758,
      "learning_rate": 0.00011556016597510374,
      "loss": 3.3909,
      "step": 820
    },
    {
      "epoch": 0.4304560630125442,
      "grad_norm": 5.160090923309326,
      "learning_rate": 0.00011452282157676349,
      "loss": 3.4949,
      "step": 830
    },
    {
      "epoch": 0.4356422806392013,
      "grad_norm": 4.1030988693237305,
      "learning_rate": 0.00011348547717842324,
      "loss": 3.3909,
      "step": 840
    },
    {
      "epoch": 0.44082849826585846,
      "grad_norm": 4.673760414123535,
      "learning_rate": 0.000112448132780083,
      "loss": 3.3669,
      "step": 850
    },
    {
      "epoch": 0.4460147158925156,
      "grad_norm": 4.806177616119385,
      "learning_rate": 0.00011141078838174275,
      "loss": 3.4535,
      "step": 860
    },
    {
      "epoch": 0.4512009335191728,
      "grad_norm": 3.644047498703003,
      "learning_rate": 0.0001103734439834025,
      "loss": 3.4024,
      "step": 870
    },
    {
      "epoch": 0.45638715114582995,
      "grad_norm": 4.11937952041626,
      "learning_rate": 0.00010933609958506224,
      "loss": 3.4126,
      "step": 880
    },
    {
      "epoch": 0.4615733687724871,
      "grad_norm": 3.893165349960327,
      "learning_rate": 0.000108298755186722,
      "loss": 3.3531,
      "step": 890
    },
    {
      "epoch": 0.4667595863991443,
      "grad_norm": 4.7134928703308105,
      "learning_rate": 0.00010726141078838175,
      "loss": 3.344,
      "step": 900
    },
    {
      "epoch": 0.47194580402580144,
      "grad_norm": 5.205343246459961,
      "learning_rate": 0.0001062240663900415,
      "loss": 3.5408,
      "step": 910
    },
    {
      "epoch": 0.4771320216524586,
      "grad_norm": 4.053923606872559,
      "learning_rate": 0.00010518672199170125,
      "loss": 3.454,
      "step": 920
    },
    {
      "epoch": 0.48231823927911577,
      "grad_norm": 4.432187080383301,
      "learning_rate": 0.000104149377593361,
      "loss": 3.4274,
      "step": 930
    },
    {
      "epoch": 0.48750445690577293,
      "grad_norm": 4.3982319831848145,
      "learning_rate": 0.00010311203319502075,
      "loss": 3.394,
      "step": 940
    },
    {
      "epoch": 0.49269067453243004,
      "grad_norm": 4.7568817138671875,
      "learning_rate": 0.0001020746887966805,
      "loss": 3.366,
      "step": 950
    },
    {
      "epoch": 0.4978768921590872,
      "grad_norm": 5.522087097167969,
      "learning_rate": 0.00010103734439834025,
      "loss": 3.4634,
      "step": 960
    },
    {
      "epoch": 0.5030631097857444,
      "grad_norm": 4.2074503898620605,
      "learning_rate": 0.0001,
      "loss": 3.4871,
      "step": 970
    },
    {
      "epoch": 0.5082493274124016,
      "grad_norm": 3.9563112258911133,
      "learning_rate": 9.896265560165976e-05,
      "loss": 3.338,
      "step": 980
    },
    {
      "epoch": 0.5134355450390587,
      "grad_norm": 4.420141220092773,
      "learning_rate": 9.79253112033195e-05,
      "loss": 3.3455,
      "step": 990
    },
    {
      "epoch": 0.5186217626657159,
      "grad_norm": 3.8053548336029053,
      "learning_rate": 9.688796680497926e-05,
      "loss": 3.4559,
      "step": 1000
    },
    {
      "epoch": 0.523807980292373,
      "grad_norm": 3.9689605236053467,
      "learning_rate": 9.585062240663902e-05,
      "loss": 3.4199,
      "step": 1010
    },
    {
      "epoch": 0.5289941979190301,
      "grad_norm": 4.41328763961792,
      "learning_rate": 9.481327800829876e-05,
      "loss": 3.426,
      "step": 1020
    },
    {
      "epoch": 0.5341804155456873,
      "grad_norm": 3.6807920932769775,
      "learning_rate": 9.377593360995851e-05,
      "loss": 3.4524,
      "step": 1030
    },
    {
      "epoch": 0.5393666331723445,
      "grad_norm": 4.213167190551758,
      "learning_rate": 9.273858921161826e-05,
      "loss": 3.4302,
      "step": 1040
    },
    {
      "epoch": 0.5445528507990016,
      "grad_norm": 4.970890045166016,
      "learning_rate": 9.1701244813278e-05,
      "loss": 3.2913,
      "step": 1050
    },
    {
      "epoch": 0.5497390684256588,
      "grad_norm": 5.120076656341553,
      "learning_rate": 9.066390041493777e-05,
      "loss": 3.378,
      "step": 1060
    },
    {
      "epoch": 0.554925286052316,
      "grad_norm": 3.771559715270996,
      "learning_rate": 8.962655601659752e-05,
      "loss": 3.3351,
      "step": 1070
    },
    {
      "epoch": 0.5601115036789731,
      "grad_norm": 4.377884387969971,
      "learning_rate": 8.858921161825726e-05,
      "loss": 3.3376,
      "step": 1080
    },
    {
      "epoch": 0.5652977213056303,
      "grad_norm": 3.7393746376037598,
      "learning_rate": 8.755186721991701e-05,
      "loss": 3.4011,
      "step": 1090
    },
    {
      "epoch": 0.5704839389322874,
      "grad_norm": 4.381932735443115,
      "learning_rate": 8.651452282157677e-05,
      "loss": 3.2617,
      "step": 1100
    },
    {
      "epoch": 0.5756701565589446,
      "grad_norm": 4.100915431976318,
      "learning_rate": 8.547717842323652e-05,
      "loss": 3.3494,
      "step": 1110
    },
    {
      "epoch": 0.5808563741856018,
      "grad_norm": 4.332756996154785,
      "learning_rate": 8.443983402489627e-05,
      "loss": 3.3068,
      "step": 1120
    },
    {
      "epoch": 0.5860425918122589,
      "grad_norm": 4.400269031524658,
      "learning_rate": 8.340248962655603e-05,
      "loss": 3.4743,
      "step": 1130
    },
    {
      "epoch": 0.5912288094389161,
      "grad_norm": 4.6709885597229,
      "learning_rate": 8.236514522821577e-05,
      "loss": 3.3705,
      "step": 1140
    },
    {
      "epoch": 0.5964150270655733,
      "grad_norm": 4.202683448791504,
      "learning_rate": 8.132780082987552e-05,
      "loss": 3.3754,
      "step": 1150
    },
    {
      "epoch": 0.6016012446922304,
      "grad_norm": 4.129523754119873,
      "learning_rate": 8.029045643153527e-05,
      "loss": 3.3416,
      "step": 1160
    },
    {
      "epoch": 0.6067874623188876,
      "grad_norm": 3.973665952682495,
      "learning_rate": 7.925311203319503e-05,
      "loss": 3.3605,
      "step": 1170
    },
    {
      "epoch": 0.6119736799455447,
      "grad_norm": 4.093103408813477,
      "learning_rate": 7.821576763485478e-05,
      "loss": 3.3626,
      "step": 1180
    },
    {
      "epoch": 0.6171598975722019,
      "grad_norm": 4.053674221038818,
      "learning_rate": 7.717842323651453e-05,
      "loss": 3.4398,
      "step": 1190
    },
    {
      "epoch": 0.6223461151988591,
      "grad_norm": 4.817799091339111,
      "learning_rate": 7.614107883817427e-05,
      "loss": 3.2866,
      "step": 1200
    },
    {
      "epoch": 0.6275323328255162,
      "grad_norm": 5.323470115661621,
      "learning_rate": 7.510373443983402e-05,
      "loss": 3.3392,
      "step": 1210
    },
    {
      "epoch": 0.6327185504521734,
      "grad_norm": 3.993246555328369,
      "learning_rate": 7.406639004149378e-05,
      "loss": 3.3914,
      "step": 1220
    },
    {
      "epoch": 0.6379047680788305,
      "grad_norm": 4.184848308563232,
      "learning_rate": 7.302904564315353e-05,
      "loss": 3.3974,
      "step": 1230
    },
    {
      "epoch": 0.6430909857054876,
      "grad_norm": 3.397195339202881,
      "learning_rate": 7.199170124481328e-05,
      "loss": 3.2815,
      "step": 1240
    },
    {
      "epoch": 0.6482772033321448,
      "grad_norm": 4.087735652923584,
      "learning_rate": 7.095435684647304e-05,
      "loss": 3.2907,
      "step": 1250
    },
    {
      "epoch": 0.6534634209588019,
      "grad_norm": 4.631424427032471,
      "learning_rate": 6.991701244813278e-05,
      "loss": 3.4069,
      "step": 1260
    },
    {
      "epoch": 0.6586496385854591,
      "grad_norm": 4.171383857727051,
      "learning_rate": 6.887966804979253e-05,
      "loss": 3.2199,
      "step": 1270
    },
    {
      "epoch": 0.6638358562121163,
      "grad_norm": 4.824154853820801,
      "learning_rate": 6.784232365145228e-05,
      "loss": 3.3011,
      "step": 1280
    },
    {
      "epoch": 0.6690220738387734,
      "grad_norm": 3.7608370780944824,
      "learning_rate": 6.680497925311204e-05,
      "loss": 3.3963,
      "step": 1290
    },
    {
      "epoch": 0.6742082914654306,
      "grad_norm": 4.104073524475098,
      "learning_rate": 6.576763485477179e-05,
      "loss": 3.2315,
      "step": 1300
    },
    {
      "epoch": 0.6793945090920878,
      "grad_norm": 4.561061382293701,
      "learning_rate": 6.473029045643154e-05,
      "loss": 3.3515,
      "step": 1310
    },
    {
      "epoch": 0.6845807267187449,
      "grad_norm": 4.7698540687561035,
      "learning_rate": 6.369294605809128e-05,
      "loss": 3.3597,
      "step": 1320
    },
    {
      "epoch": 0.6897669443454021,
      "grad_norm": 3.6736690998077393,
      "learning_rate": 6.265560165975103e-05,
      "loss": 3.232,
      "step": 1330
    },
    {
      "epoch": 0.6949531619720593,
      "grad_norm": 3.95798921585083,
      "learning_rate": 6.161825726141079e-05,
      "loss": 3.33,
      "step": 1340
    },
    {
      "epoch": 0.7001393795987164,
      "grad_norm": 4.9558234214782715,
      "learning_rate": 6.058091286307054e-05,
      "loss": 3.3462,
      "step": 1350
    },
    {
      "epoch": 0.7053255972253736,
      "grad_norm": 4.492739677429199,
      "learning_rate": 5.9543568464730294e-05,
      "loss": 3.2853,
      "step": 1360
    },
    {
      "epoch": 0.7105118148520307,
      "grad_norm": 4.54218053817749,
      "learning_rate": 5.850622406639005e-05,
      "loss": 3.3394,
      "step": 1370
    },
    {
      "epoch": 0.7156980324786879,
      "grad_norm": 4.060050010681152,
      "learning_rate": 5.7468879668049794e-05,
      "loss": 3.394,
      "step": 1380
    },
    {
      "epoch": 0.7208842501053451,
      "grad_norm": 4.61298942565918,
      "learning_rate": 5.643153526970955e-05,
      "loss": 3.3506,
      "step": 1390
    },
    {
      "epoch": 0.7260704677320022,
      "grad_norm": 4.001279354095459,
      "learning_rate": 5.53941908713693e-05,
      "loss": 3.3173,
      "step": 1400
    },
    {
      "epoch": 0.7312566853586594,
      "grad_norm": 4.326908588409424,
      "learning_rate": 5.4356846473029046e-05,
      "loss": 3.3564,
      "step": 1410
    },
    {
      "epoch": 0.7364429029853166,
      "grad_norm": 4.41157865524292,
      "learning_rate": 5.33195020746888e-05,
      "loss": 3.3425,
      "step": 1420
    },
    {
      "epoch": 0.7416291206119737,
      "grad_norm": 4.764340400695801,
      "learning_rate": 5.228215767634855e-05,
      "loss": 3.3299,
      "step": 1430
    },
    {
      "epoch": 0.7468153382386309,
      "grad_norm": 4.767736911773682,
      "learning_rate": 5.12448132780083e-05,
      "loss": 3.4028,
      "step": 1440
    },
    {
      "epoch": 0.752001555865288,
      "grad_norm": 4.14430570602417,
      "learning_rate": 5.020746887966805e-05,
      "loss": 3.3471,
      "step": 1450
    },
    {
      "epoch": 0.7571877734919451,
      "grad_norm": 4.058429718017578,
      "learning_rate": 4.91701244813278e-05,
      "loss": 3.3662,
      "step": 1460
    },
    {
      "epoch": 0.7623739911186023,
      "grad_norm": 4.566441535949707,
      "learning_rate": 4.813278008298756e-05,
      "loss": 3.3302,
      "step": 1470
    },
    {
      "epoch": 0.7675602087452594,
      "grad_norm": 5.210699558258057,
      "learning_rate": 4.7095435684647304e-05,
      "loss": 3.3437,
      "step": 1480
    },
    {
      "epoch": 0.7727464263719166,
      "grad_norm": 4.392273902893066,
      "learning_rate": 4.605809128630705e-05,
      "loss": 3.2375,
      "step": 1490
    },
    {
      "epoch": 0.7779326439985738,
      "grad_norm": 4.105725288391113,
      "learning_rate": 4.502074688796681e-05,
      "loss": 3.3283,
      "step": 1500
    },
    {
      "epoch": 0.7831188616252309,
      "grad_norm": 4.29960298538208,
      "learning_rate": 4.398340248962656e-05,
      "loss": 3.2589,
      "step": 1510
    },
    {
      "epoch": 0.7883050792518881,
      "grad_norm": 5.8197102546691895,
      "learning_rate": 4.29460580912863e-05,
      "loss": 3.3652,
      "step": 1520
    },
    {
      "epoch": 0.7934912968785452,
      "grad_norm": 4.898454666137695,
      "learning_rate": 4.190871369294606e-05,
      "loss": 3.3473,
      "step": 1530
    },
    {
      "epoch": 0.7986775145052024,
      "grad_norm": 4.571974754333496,
      "learning_rate": 4.087136929460581e-05,
      "loss": 3.2886,
      "step": 1540
    },
    {
      "epoch": 0.8038637321318596,
      "grad_norm": 5.001023292541504,
      "learning_rate": 3.983402489626556e-05,
      "loss": 3.3229,
      "step": 1550
    },
    {
      "epoch": 0.8090499497585167,
      "grad_norm": 4.051759719848633,
      "learning_rate": 3.8796680497925316e-05,
      "loss": 3.3293,
      "step": 1560
    },
    {
      "epoch": 0.8142361673851739,
      "grad_norm": 4.808770656585693,
      "learning_rate": 3.775933609958506e-05,
      "loss": 3.4739,
      "step": 1570
    },
    {
      "epoch": 0.8194223850118311,
      "grad_norm": 4.04315710067749,
      "learning_rate": 3.6721991701244815e-05,
      "loss": 3.3949,
      "step": 1580
    },
    {
      "epoch": 0.8246086026384882,
      "grad_norm": 6.507344722747803,
      "learning_rate": 3.568464730290457e-05,
      "loss": 3.3068,
      "step": 1590
    },
    {
      "epoch": 0.8297948202651454,
      "grad_norm": 4.261739253997803,
      "learning_rate": 3.4647302904564314e-05,
      "loss": 3.2617,
      "step": 1600
    },
    {
      "epoch": 0.8349810378918026,
      "grad_norm": 4.021373271942139,
      "learning_rate": 3.360995850622407e-05,
      "loss": 3.3458,
      "step": 1610
    },
    {
      "epoch": 0.8401672555184597,
      "grad_norm": 4.227076053619385,
      "learning_rate": 3.257261410788382e-05,
      "loss": 3.3228,
      "step": 1620
    },
    {
      "epoch": 0.8453534731451169,
      "grad_norm": 4.316836833953857,
      "learning_rate": 3.153526970954357e-05,
      "loss": 3.337,
      "step": 1630
    },
    {
      "epoch": 0.850539690771774,
      "grad_norm": 4.287412643432617,
      "learning_rate": 3.0497925311203323e-05,
      "loss": 3.3428,
      "step": 1640
    },
    {
      "epoch": 0.8557259083984312,
      "grad_norm": 4.433019161224365,
      "learning_rate": 2.9460580912863073e-05,
      "loss": 3.2888,
      "step": 1650
    },
    {
      "epoch": 0.8609121260250884,
      "grad_norm": 4.449275970458984,
      "learning_rate": 2.8423236514522823e-05,
      "loss": 3.2349,
      "step": 1660
    },
    {
      "epoch": 0.8660983436517455,
      "grad_norm": 5.977766990661621,
      "learning_rate": 2.7385892116182576e-05,
      "loss": 3.3521,
      "step": 1670
    },
    {
      "epoch": 0.8712845612784026,
      "grad_norm": 5.218619346618652,
      "learning_rate": 2.6348547717842326e-05,
      "loss": 3.3293,
      "step": 1680
    },
    {
      "epoch": 0.8764707789050598,
      "grad_norm": 4.743282318115234,
      "learning_rate": 2.5311203319502075e-05,
      "loss": 3.3833,
      "step": 1690
    },
    {
      "epoch": 0.8816569965317169,
      "grad_norm": 4.258573532104492,
      "learning_rate": 2.427385892116183e-05,
      "loss": 3.298,
      "step": 1700
    },
    {
      "epoch": 0.8868432141583741,
      "grad_norm": 4.050993919372559,
      "learning_rate": 2.3236514522821578e-05,
      "loss": 3.2563,
      "step": 1710
    },
    {
      "epoch": 0.8920294317850312,
      "grad_norm": 4.4106831550598145,
      "learning_rate": 2.2199170124481328e-05,
      "loss": 3.414,
      "step": 1720
    },
    {
      "epoch": 0.8972156494116884,
      "grad_norm": 4.682057857513428,
      "learning_rate": 2.116182572614108e-05,
      "loss": 3.3691,
      "step": 1730
    },
    {
      "epoch": 0.9024018670383456,
      "grad_norm": 4.6049323081970215,
      "learning_rate": 2.012448132780083e-05,
      "loss": 3.3383,
      "step": 1740
    },
    {
      "epoch": 0.9075880846650027,
      "grad_norm": 4.723843574523926,
      "learning_rate": 1.908713692946058e-05,
      "loss": 3.4003,
      "step": 1750
    },
    {
      "epoch": 0.9127743022916599,
      "grad_norm": 5.075375080108643,
      "learning_rate": 1.8049792531120333e-05,
      "loss": 3.3391,
      "step": 1760
    },
    {
      "epoch": 0.9179605199183171,
      "grad_norm": 3.974458932876587,
      "learning_rate": 1.7012448132780083e-05,
      "loss": 3.2793,
      "step": 1770
    },
    {
      "epoch": 0.9231467375449742,
      "grad_norm": 5.088917255401611,
      "learning_rate": 1.5975103734439833e-05,
      "loss": 3.2941,
      "step": 1780
    },
    {
      "epoch": 0.9283329551716314,
      "grad_norm": 4.949202537536621,
      "learning_rate": 1.4937759336099586e-05,
      "loss": 3.3687,
      "step": 1790
    },
    {
      "epoch": 0.9335191727982886,
      "grad_norm": 4.669493198394775,
      "learning_rate": 1.3900414937759337e-05,
      "loss": 3.4503,
      "step": 1800
    },
    {
      "epoch": 0.9387053904249457,
      "grad_norm": 4.550719261169434,
      "learning_rate": 1.2863070539419087e-05,
      "loss": 3.3325,
      "step": 1810
    },
    {
      "epoch": 0.9438916080516029,
      "grad_norm": 4.9514546394348145,
      "learning_rate": 1.1825726141078838e-05,
      "loss": 3.311,
      "step": 1820
    },
    {
      "epoch": 0.94907782567826,
      "grad_norm": 4.149371147155762,
      "learning_rate": 1.078838174273859e-05,
      "loss": 3.2838,
      "step": 1830
    },
    {
      "epoch": 0.9542640433049172,
      "grad_norm": 4.077359199523926,
      "learning_rate": 9.751037344398341e-06,
      "loss": 3.2853,
      "step": 1840
    },
    {
      "epoch": 0.9594502609315744,
      "grad_norm": 5.130097389221191,
      "learning_rate": 8.713692946058091e-06,
      "loss": 3.3721,
      "step": 1850
    },
    {
      "epoch": 0.9646364785582315,
      "grad_norm": 4.130520820617676,
      "learning_rate": 7.676348547717842e-06,
      "loss": 3.1827,
      "step": 1860
    },
    {
      "epoch": 0.9698226961848887,
      "grad_norm": 5.065347194671631,
      "learning_rate": 6.639004149377594e-06,
      "loss": 3.26,
      "step": 1870
    },
    {
      "epoch": 0.9750089138115459,
      "grad_norm": 4.830158710479736,
      "learning_rate": 5.601659751037345e-06,
      "loss": 3.3042,
      "step": 1880
    },
    {
      "epoch": 0.980195131438203,
      "grad_norm": 4.16216516494751,
      "learning_rate": 4.564315352697096e-06,
      "loss": 3.3732,
      "step": 1890
    },
    {
      "epoch": 0.9853813490648601,
      "grad_norm": 4.462958335876465,
      "learning_rate": 3.5269709543568467e-06,
      "loss": 3.3902,
      "step": 1900
    },
    {
      "epoch": 0.9905675666915172,
      "grad_norm": 4.9871368408203125,
      "learning_rate": 2.4896265560165977e-06,
      "loss": 3.2977,
      "step": 1910
    },
    {
      "epoch": 0.9957537843181744,
      "grad_norm": 4.727348804473877,
      "learning_rate": 1.4522821576763486e-06,
      "loss": 3.2982,
      "step": 1920
    }
  ],
  "logging_steps": 10,
  "max_steps": 1928,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 1,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 1.6862199913906176e+17,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
